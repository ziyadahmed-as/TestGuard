# Generated by Django 5.2.5 on 2025-09-07 11:43

import django.core.validators
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='AssessmentTrend',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('period_start', models.DateTimeField(help_text='Start of the analysis period')),
                ('period_end', models.DateTimeField(help_text='End of the analysis period')),
                ('average_score', models.DecimalField(decimal_places=2, help_text='Mean score across all attempts during this period', max_digits=5)),
                ('pass_rate', models.DecimalField(decimal_places=2, help_text='Percentage of passing attempts during this period', max_digits=5, validators=[django.core.validators.MinValueValidator(0), django.core.validators.MaxValueValidator(100)])),
                ('common_misconceptions', models.JSONField(default=dict, help_text='Frequently identified knowledge gaps and misunderstanding patterns')),
                ('question_difficulty', models.JSONField(default=dict, help_text='Difficulty analysis for individual exam questions')),
                ('generated_at', models.DateTimeField(auto_now_add=True)),
            ],
            options={
                'verbose_name': 'Assessment Trend',
                'verbose_name_plural': 'Assessment Trends',
                'ordering': ['-period_end'],
            },
        ),
        migrations.CreateModel(
            name='Feedback',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('comment', models.TextField(help_text='Comprehensive overall commentary on response quality and performance')),
                ('criterion_feedback', models.JSONField(default=dict, help_text='Targeted feedback for individual rubric criteria with specific observations')),
                ('suggested_improvement', models.TextField(blank=True, help_text='Actionable recommendations for future development and enhancement')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
            options={
                'verbose_name': 'Assessment Feedback',
                'verbose_name_plural': 'Assessment Feedback',
                'ordering': ['-created_at'],
            },
        ),
        migrations.CreateModel(
            name='GradingAnalytics',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('total_manual_questions', models.PositiveIntegerField(default=0, help_text='Total number of questions requiring manual evaluation')),
                ('graded_questions', models.PositiveIntegerField(default=0, help_text='Number of questions that have been completely assessed')),
                ('average_grading_time', models.DecimalField(blank=True, decimal_places=2, help_text='Mean time in minutes to complete question evaluation', max_digits=8, null=True)),
                ('grader_performance', models.JSONField(default=dict, help_text='Productivity, consistency, and quality metrics by individual grader')),
                ('rubric_usage', models.JSONField(default=dict, help_text='Frequency distribution and application statistics of grading rubrics')),
                ('score_distribution', models.JSONField(default=dict, help_text='Statistical distribution of assigned scores across all evaluations')),
                ('generated_at', models.DateTimeField(auto_now_add=True)),
            ],
            options={
                'verbose_name': 'Grading Analytics',
                'verbose_name_plural': 'Grading Analytics',
                'ordering': ['-generated_at'],
            },
        ),
        migrations.CreateModel(
            name='GradingRubric',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('name', models.CharField(help_text='Descriptive identifier for the grading rubric', max_length=100)),
                ('description', models.TextField(blank=True, help_text="Comprehensive overview of the rubric's purpose, application, and evaluation standards")),
                ('criteria', models.JSONField(help_text='Structured evaluation criteria with point allocations and performance level descriptors')),
                ('max_score', models.DecimalField(decimal_places=2, default=0.0, help_text='Maximum achievable score derived from criteria point allocations', max_digits=5, validators=[django.core.validators.MinValueValidator(0)])),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
            options={
                'verbose_name': 'Grading Rubric',
                'verbose_name_plural': 'Grading Rubrics',
                'ordering': ['name'],
            },
        ),
        migrations.CreateModel(
            name='ManualGradingQueue',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('status', models.CharField(choices=[('PENDING', 'Pending Grading'), ('IN_PROGRESS', 'Grading in Progress'), ('COMPLETED', 'Grading Completed'), ('REVIEW_NEEDED', 'Quality Review Required')], default='PENDING', help_text='Current workflow state of the grading assignment', max_length=15)),
                ('priority', models.PositiveIntegerField(choices=[(1, 'Low'), (2, 'Medium'), (3, 'High')], default=1, help_text='Urgency level determining grading queue position')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('started_at', models.DateTimeField(blank=True, help_text='Timestamp when grading process commenced', null=True)),
                ('completed_at', models.DateTimeField(blank=True, help_text='Timestamp when grading process was finalized', null=True)),
            ],
            options={
                'verbose_name': 'Grading Queue Item',
                'verbose_name_plural': 'Grading Queue',
                'ordering': ['priority', '-created_at'],
            },
        ),
        migrations.CreateModel(
            name='PerformanceAnalytics',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('overall_score', models.DecimalField(decimal_places=2, help_text='Composite score combining all assessment components', max_digits=5, validators=[django.core.validators.MinValueValidator(0)])),
                ('auto_graded_score', models.DecimalField(decimal_places=2, default=0.0, help_text='Points earned from automatically evaluated objective questions', max_digits=5)),
                ('manually_graded_score', models.DecimalField(decimal_places=2, default=0.0, help_text='Points earned from manually evaluated subjective responses', max_digits=5)),
                ('generated_at', models.DateTimeField(auto_now_add=True)),
            ],
            options={
                'verbose_name': 'Performance Analytics',
                'verbose_name_plural': 'Performance Analytics',
                'ordering': ['-generated_at'],
            },
        ),
        migrations.CreateModel(
            name='RubricScore',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('scores', models.JSONField(help_text='Criterion-level scoring breakdown: {criterion_name: awarded_points}')),
                ('total_score', models.DecimalField(decimal_places=2, help_text='Automatically calculated aggregate score from individual criterion evaluations', max_digits=5, validators=[django.core.validators.MinValueValidator(0)])),
                ('feedback_comments', models.TextField(blank=True, help_text='Comprehensive qualitative feedback on response quality and areas for improvement')),
                ('graded_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
            ],
            options={
                'verbose_name': 'Rubric Score',
                'verbose_name_plural': 'Rubric Scores',
                'ordering': ['-graded_at'],
            },
        ),
    ]
